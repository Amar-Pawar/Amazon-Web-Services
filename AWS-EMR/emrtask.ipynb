{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "*** @Author: Amar Pawar ***\n",
    "\n",
    "*** @Date: 2021-09-03 ***\n",
    "\n",
    "*** @Last Modified by: Amar Pawar ***\n",
    "\n",
    "*** @Last Modified time: 2021-09-03 ***\n",
    "\n",
    "*** @Title : AWS EMR+PySpark ***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('operations').getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "sc.setLogLevel('ERROR')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3://amartest1/*.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "user_data = file_data_df.groupBy(\"user_name\").count()\n",
    "user_data.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n",
      "|           user_name|count|\n",
      "+--------------------+-----+\n",
      "|salinabodale73@gm...|  569|\n",
      "|sharlawar77@gmail...|  580|\n",
      "|rahilstar11@gmail...|  551|\n",
      "|deepshukla292@gma...|  565|\n",
      "|  iamnzm@outlook.com|  614|\n",
      "|markfernandes66@g...|  508|\n",
      "|damodharn21@gmail...|  253|\n",
      "|bhagyashrichalke2...|  482|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "user_data7 = file_data_df.select(\"user_name\").where((file_data_df.keyboard != 0.0) | (file_data_df.mouse != 0.0))\n",
    "user_data8 = user_data7.groupBy(\"user_name\").count()\n",
    "work_seconds = user_data8.withColumn('work_seconds',  ( ( (user_data8['count'] - 1) * 5) * 60)/ 6)\n",
    "highest_avg_hour = work_seconds.withColumn(\"average_hours\", concat(\n",
    "            floor(col(\"work_seconds\") % 86400 / 3600), lit(\":\"),\n",
    "            floor((col(\"work_seconds\") % 86400) % 3600 / 60), lit(\"\"),\n",
    "           \n",
    "        ))\\\n",
    "    .drop(\"work_seconds\")\n",
    "highest_avg_hour.createOrReplaceTempView(\"sql_view\")\n",
    "highest_avg_hour1 = spark.sql(\"SELECT user_name, average_hours FROM sql_view ORDER BY average_hours desc\")\n",
    "highest_avg_hour.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-02 16:03:42,369 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+-------------+\n",
      "|           user_name|count|average_hours|\n",
      "+--------------------+-----+-------------+\n",
      "|salinabodale73@gm...|  440|          6:5|\n",
      "|sharlawar77@gmail...|  457|         6:20|\n",
      "|rahilstar11@gmail...|  399|         5:31|\n",
      "|deepshukla292@gma...|  475|         6:35|\n",
      "|  iamnzm@outlook.com|  459|         6:21|\n",
      "|markfernandes66@g...|  389|         5:23|\n",
      "|damodharn21@gmail...|  191|         2:38|\n",
      "|bhagyashrichalke2...|  361|          5:0|\n",
      "+--------------------+-----+-------------+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "highest_avg_hour.createOrReplaceTempView(\"sql_view1\")\n",
    "lowest_avg_hour = spark.sql(\"SELECT user_name, count, average_hours FROM sql_view1 ORDER BY average_hours asc\")\n",
    "lowest_avg_hour.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+-------------+\n",
      "|           user_name|count|average_hours|\n",
      "+--------------------+-----+-------------+\n",
      "|damodharn21@gmail...|  191|         2:38|\n",
      "|bhagyashrichalke2...|  361|          5:0|\n",
      "|markfernandes66@g...|  389|         5:23|\n",
      "|rahilstar11@gmail...|  399|         5:31|\n",
      "|sharlawar77@gmail...|  457|         6:20|\n",
      "|  iamnzm@outlook.com|  459|         6:21|\n",
      "|deepshukla292@gma...|  475|         6:35|\n",
      "|salinabodale73@gm...|  440|          6:5|\n",
      "+--------------------+-----+-------------+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "columns1 = ['user_name', 'keyboard', 'mouse']\n",
    "user_data5 = file_data_df.select(\"user_name\",\"keyboard\",\"mouse\").where((file_data_df.keyboard==0)&(file_data_df.mouse==0)).toDF(*columns1)\n",
    "user_data6 = user_data5.groupBy(\"user_name\").count()\n",
    "idle_minit = user_data6.withColumn('idle_minits',  ( ((user_data6['count'] - 1) * 5) * 60 / 6) )  \n",
    "idle_hour = idle_minit.withColumn(\"idle_hours\", concat(\n",
    "            floor(col(\"idle_minits\") % 86400 / 3600), lit(\":\"),\n",
    "            floor((col(\"idle_minits\") % 86400) % 3600 / 60), lit(\"\"),\n",
    "           \n",
    "        ))\\\n",
    "    .drop(\"idle_minits\")\\\n",
    "    .sort(desc(\"idle_hours\"))\n",
    "idle_hour.show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------------+-----+----------+\n",
      "|user_name                   |count|idle_hours|\n",
      "+----------------------------+-----+----------+\n",
      "|iamnzm@outlook.com          |155  |2:8       |\n",
      "|rahilstar11@gmail.com       |152  |2:5       |\n",
      "|salinabodale73@gmail.com    |133  |1:50      |\n",
      "|sharlawar77@gmail.com       |123  |1:41      |\n",
      "|bhagyashrichalke21@gmail.com|121  |1:40      |\n",
      "|markfernandes66@gmail.com   |119  |1:38      |\n",
      "|deepshukla292@gmail.com     |90   |1:14      |\n",
      "|damodharn21@gmail.com       |62   |0:50      |\n",
      "+----------------------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}